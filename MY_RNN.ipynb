{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "PATH='data/nietzsche/'\n",
    "get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "text = open(f'{PATH}nietzsche.txt', encoding=\"utf-8\").read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique 85\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(list(set(text)))\n",
    "vocab_size = len(vocab_list)+1\n",
    "print(\"Number of unique\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list.insert(0, '\\0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char:i for i, char in enumerate(vocab_list)}\n",
    "idx_to_char = {i: char for i, char in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_text = [char_to_idx[char] for char in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 3\n",
    "every_char1 = [indexed_text[i] for i in range(0, len(indexed_text) - step, step)]\n",
    "every_char2 = [indexed_text[i + 1] for i in range(0, len(indexed_text) - step, step)]\n",
    "every_char3 = [indexed_text[i + 2] for i in range(0, len(indexed_text) - step, step)]\n",
    "output = [indexed_text[i + 3] for i in range(0, len(indexed_text) - step, step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = np.stack(every_char1)\n",
    "in2 = np.stack(every_char2)\n",
    "in3 = np.stack(every_char3)\n",
    "output = np.stack(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 3 char model\n",
    "1. Create inputs (ie. create a list of every 4th character starting at 0th, 1st, 2nd)\n",
    "1. Create & train model (ie. Char3Model class - nn.Embedding, nn.Linear)\n",
    "1. self.e -> self.linear_input -> F.relu x3\n",
    "1. h+input -> self.linear_hidden -> F.tanh\n",
    "1. ColumnarModelData.from_arrays\n",
    "1. optim.Adam\n",
    "1. fit()\n",
    "1. get_next() - test model by getting next char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "1. Create & train model (ie. Char3Model class - nn.Embedding, nn.Linear)\n",
    "1. self.e -> self.linear_input -> F.relu x3\n",
    "1. h+input -> self.linear_hidden -> F.tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256\n",
    "n_factors = 42\n",
    "\n",
    "class Char3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.linear_input = nn.Linear(n_factors, n_hidden)\n",
    "        self.linear_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_output = nn.Linear(n_hidden, vocab_size)\n",
    "    def forward(self, x1, x2, x3):\n",
    "        char1embed = self.e(x1)\n",
    "        char2embed = self.e(x2)\n",
    "        char3embed = self.e(x3)\n",
    "        \n",
    "        in1 = F.relu(self.linear_input(char1embed))\n",
    "        in2 = F.relu(self.linear_input(char2embed))\n",
    "        in3 = F.relu(self.linear_input(char3embed))\n",
    "        \n",
    "        hidden_state = V(torch.zeros(in1.size()).cuda())\n",
    "        hidden_state = F.tanh(self.linear_hidden(hidden_state + in1))\n",
    "        hidden_state = F.tanh(self.linear_hidden(hidden_state + in2))\n",
    "        hidden_state = F.tanh(self.linear_hidden(hidden_state + in2))\n",
    "        \n",
    "        return F.log_softmax(self.linear_output(hidden_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = ColumnarModelData.from_arrays('.', [-1], np.stack([in1, in2, in3], axis=1), output, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Char3Model(len(vocab_list), n_factors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b359053517b42fa9851641a004e0e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      2.833131   2.505615  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.50561])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(optimizer, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a5023d6201443387912f17e362f5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      2.647739   1.466757  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.46676])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inputString):\n",
    "    indexes = T(np.array([char_to_idx[c] for c in inputString]))\n",
    "    output = model(*VV(indexes))\n",
    "    i = np.argmax(to_np(output))\n",
    "    return idx_to_char[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "print(get_next(' th'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN\n",
    "1. Loop through and create a list of every 8th character - input char data from series 0-7 \n",
    "1. Loop through and create a list of the next character in this series (ie. 9th character)\n",
    "1. Create CharLoopModel -  nn.Embedding, nn.Linear, initialise torch.zeros(bs, n_hidden).cuda(), for c in cs\n",
    "1. self.e -> self.linear_in -> F.relu, h+input -> linear_hidden -> tanh\n",
    "1. Create CharLoopConcatModel - torch.cat((h, self.e(c)), 1)\n",
    "1. Test model with get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 8\n",
    "train_chars = [[indexed_text[i+j] for j in range(step)] for i in range(len(indexed_text) - step)]\n",
    "test_chars = [indexed_text[j+step] for j in range(len(indexed_text) - step) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chars = np.stack(train_chars)\n",
    "test_chars = np.stack(test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(train_chars) - step - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = ColumnarModelData.from_arrays('.', val_idx, train_chars, test_chars, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.linear_input = nn.Linear(n_factors, n_hidden)\n",
    "        self.linear_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_output = nn.Linear(n_hidden, vocab_size)\n",
    "    def forward(self, *chars):\n",
    "        batch_size = chars[0].size(0)\n",
    "        hidden_state = V(torch.zeros(batch_size, n_hidden).cuda())\n",
    "        for char in chars:\n",
    "            input_activation = F.relu(self.linear_input(self.e(char)))\n",
    "            hidden_state = F.tanh(self.linear_hidden(hidden_state + input_activation))\n",
    "        return F.log_softmax(self.linear_output(hidden_state), dim=-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharLoopModel(vocab_size, n_factors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0594d832584d9aace2bc7f92811319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.999152   1.989036  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.98904])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model,model_data,1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e693dc03d84e6996b9cabd5312e73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.701737   1.69438   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.69438])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(optimizer, 1e-3)\n",
    "fit(model,model_data,1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.linear_input = nn.Linear(n_factors + n_hidden, n_hidden)\n",
    "        self.linear_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.linear_output = nn.Linear(n_hidden, vocab_size)\n",
    "    def forward(self, *chars):\n",
    "        batch_size = chars[0].size(0)\n",
    "        hidden_state = V(torch.zeros(batch_size, n_hidden).cuda())\n",
    "        for char in chars:\n",
    "            inp = torch.cat((hidden_state, self.e(char)), 1)\n",
    "            inp = F.relu(self.linear_input(inp))\n",
    "            hidden_state = F.tanh(self.linear_hidden(inp))\n",
    "        return F.log_softmax(self.linear_output(hidden_state), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharConcatModel(vocab_size, n_factors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133e249395a84908a26dac33623576d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.801922   1.782424  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.78242])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model,model_data, 1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN with Pytorch\n",
    "1. Create CharRnn (self.rnn = nn.RNN()) - embedding, RNN, linear\n",
    "1. torch.zeros() to make initialised hidden layer\n",
    "1. self.e(torch.stack(cs))\n",
    "1. self.rnn(input, hidden)\n",
    "1. self.linear_output -> F.log_softmax\n",
    "1. Test model with get_next - pass indexed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.rnn = nn.RNN(n_factors, n_hidden)\n",
    "        self.linear_output = nn.Linear(n_hidden, vocab_size)\n",
    "    def forward(self, *chars):\n",
    "        batch_size = chars[0].size(0)\n",
    "        hidden_state = V(torch.zeros(1, batch_size, n_hidden ))\n",
    "        inp = self.e(torch.stack(chars))\n",
    "        output, hidden_state = self.rnn(inp, hidden_state)\n",
    "        return F.log_softmax(self.linear_output(output[-1]), dim=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(vocab_size, n_factors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf7526008534abf9aef9890b7eee617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.869353   1.840322  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.84032])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 1, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f78ee0a4f68432493d1354117741fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.774457   1.779656  \n",
      "    1      1.752325   1.753175                              \n",
      "    2      1.727903   1.730656                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.73066])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(optimizer, 1e-4)\n",
    "fit(model, model_data, 3, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(chars):\n",
    "    indexed_char = T(np.array([char_to_idx[char] for char in chars]))\n",
    "    outp = model(*VV(indexed_char))\n",
    "    i = np.argmax(to_np(outp))\n",
    "    return idx_to_char[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(\"for thos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(chars, n):\n",
    "    res = chars\n",
    "    for i in range(n):\n",
    "        predicted = get_next(chars)\n",
    "        res = res + predicted\n",
    "        chars = chars[1:] + predicted\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those is the strenger the self-consci'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n(\"for thos\", 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-output model\n",
    "\n",
    "1. Get non-overlapping sets of chars\n",
    "1. [[idx[i + j] for i in range(cs)]for j in range (0, up to -1 idx, step size]\n",
    "1. ColumnerModelData.from_arrays()\n",
    "1. Create CharSeqRnn - self.e(torch.stack(cs)) -> self.rnn() -> self.linear_output -> F.log_softmax\n",
    "1. Define negative log loss (nll) - F.nll_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 8\n",
    "train_chars = [[indexed_text[i+j] for i in range(step)] for j in range(0, len(indexed_text) - step - 1, step)]\n",
    "test_chars = [[indexed_text[i+j] for i in range(step)] for j in range(1, len(indexed_text) - step, step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chars = np.stack(train_chars)\n",
    "test_chars = np.stack(test_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40 42 29 30 25 27 29  1]\n",
      " [ 1  1 43 45 40 40 39 43]\n",
      " [33 38 31  2 73 61 54 73]\n",
      " [ 2 44 71 74 73 61  2 62]\n",
      " [72  2 54  2 76 68 66 54]\n",
      " [67  9  9 76 61 54 73  2]\n",
      " [73 61 58 67 24  2 33 72]\n",
      " [ 2 73 61 58 71 58  2 67]]\n"
     ]
    }
   ],
   "source": [
    "print(train_chars[:step, :step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42 29 30 25 27 29  1  1]\n",
      " [ 1 43 45 40 40 39 43 33]\n",
      " [38 31  2 73 61 54 73  2]\n",
      " [44 71 74 73 61  2 62 72]\n",
      " [ 2 54  2 76 68 66 54 67]\n",
      " [ 9  9 76 61 54 73  2 73]\n",
      " [61 58 67 24  2 33 72  2]\n",
      " [73 61 58 71 58  2 67 68]]\n"
     ]
    }
   ],
   "source": [
    "print(test_chars[:step, :step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = get_cv_idxs(len(train_chars) - step - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = ColumnarModelData.from_arrays('.', val_idxs, train_chars, test_chars, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.rnn = nn.RNN(n_factors, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "    def forward(self, *chars):\n",
    "        bs = chars[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(chars))\n",
    "        outp, h = self.rnn(inp, h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqRNN(vocab_size, n_factors).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(model_data.trn_dl)\n",
    "*xs, yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    seq_len, bs, n_hidden = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1, n_hidden), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df19c6078a64b728c6c3deb2e098560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.598429   2.409292  \n",
      "    1      2.29071    2.200228                              \n",
      "    2      2.141052   2.089137                              \n",
      "    3      2.047998   2.014845                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.01484])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 4, optimizer, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN Types\n",
    "\n",
    "1. Create CharSeqStatefulRNN\n",
    "1. Create CharSeqStatefulRNN2\n",
    "1. Create CharSeqStatefulGRU (GRU) with nn.GRU\n",
    "1. Create CharrSeqStatefulLSTM (LSTM) with nn.LSTM\n",
    "1. Implement Cosine Annealing ([CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)])\n",
    "1. Implement get_next() & get_next_n()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  nietzsche.txt  \u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947, 55, 1, 485749)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "model_data = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(model_data.trn_dl), model_data.nt, len(model_data.trn_ds), len(model_data.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_var(h):\n",
    "    return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRNN (nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.hidden_init(bs)\n",
    "        \n",
    "    def forward(self, chars):\n",
    "        bs = chars[0].size(0)\n",
    "        if self.hidden.size(1) != bs: self.hidden_init(bs)\n",
    "        outp, h = self.rnn(self.e(chars), self.hidden)\n",
    "        self.hidden = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "\n",
    "    def hidden_init(self, bs):\n",
    "        self.hidden = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulRNN(model_data.nt, n_fac, 512).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2414b532bf3244fc913eedf82227ca83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.875788   1.859158  \n",
      "    1      1.689418   1.713159                               \n",
      "    2      1.604204   1.643834                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.64383])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 3, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f68cdc472cc4ead9ff452984a189a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.529106   1.598335  \n",
      "    1      1.52269    1.589979                               \n",
      "    2      1.511332   1.58347                                \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.58347])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(optimizer, 1e-4)\n",
    "fit(model, model_data, 3, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRNN2 (nn.Module):\n",
    "    def __init__ (self, vocab_size, n_factors, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.rnn = nn.RNNCell(n_factors, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.hidden_init(bs)\n",
    "\n",
    "    def forward(self, chars):\n",
    "        bs = chars[0].size(0)\n",
    "        if self.hidden.size(1) != bs: self.hidden_init(bs)\n",
    "        outp = []\n",
    "        o = self.hidden\n",
    "        for c in chars:\n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o)\n",
    "        self.hidden = repackage_var(o)\n",
    "        return F.log_softmax(self.l_out(torch.stack(outp)), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def hidden_init(self, bs):\n",
    "        self.hidden = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulRNN2(model_data.nt, n_fac, 512).cuda() #md.nt is len(field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c7a62dc3d847a08447d2551ac7926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.381398   1.518046  \n",
      "    1      1.37411    1.511216                               \n",
      "    2      1.36493    1.509865                               \n",
      "    3      1.357007   1.508882                               \n",
      "    4      1.361466   1.508638                               \n",
      "    5      1.35379    1.510122                               \n",
      "    6      1.345629   1.508762                               \n",
      "    7      1.348687   1.510139                               \n",
      "    8      1.340952   1.511155                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.51115])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 9, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulGRU (nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.gru = nn.GRU(n_factors, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.hidden_init(bs)\n",
    "        \n",
    "    def forward(self, chars):\n",
    "        bs = chars[0].size(0)\n",
    "        if self.hidden.size(1) != bs: self.hidden_init(bs)\n",
    "        outp, h = self.gru(self.e(chars), self.hidden)    \n",
    "        self.hidden = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def hidden_init(self, bs):\n",
    "        self.hidden = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulGRU(model_data.nt, n_fac, 512).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa877a39fdd4c149374f04fe69c8b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.740669   1.731596  \n",
      "    1      1.554523   1.585686                               \n",
      "    2      1.467744   1.518437                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.51844])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 3, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2baa8c068240df8efd5d9c07275ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.329577   1.45386   \n",
      "    1      1.306259   1.451589                               \n",
      "    2      1.294584   1.449693                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.44969])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 3, optimizer, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM (nn.Module):\n",
    "    def __init__(self, vocab_size, n_factors, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.nl = nl\n",
    "        self.e = nn.Embedding(vocab_size, n_factors)\n",
    "        self.lstm = nn.LSTM(n_factors, n_hidden, nl, dropout=0.5) #nl = number of layers\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.hidden_init(bs)\n",
    "\n",
    "    def forward(self, chars):\n",
    "        bs = chars[0].size(0)\n",
    "        if self.hidden[0].size(1) != bs: self.hidden_init(bs)\n",
    "        outp, h = self.lstm(self.e(chars), self.hidden)\n",
    "        self.hidden = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "        \n",
    "    def hidden_init(self, bs):\n",
    "        self.hidden = (V(torch.zeros(self.nl, bs, n_hidden)), \n",
    "                       V(torch.zeros(self.nl, bs, n_hidden)))\n",
    "        #hidden layer initialised as a tuple of 2 0 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharSeqStatefulLSTM(model_data.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, model, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c6478c81f94a4f85b72638a2250292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.77631    1.687049  \n",
      "    1      1.673802   1.595266                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.59527])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, model_data, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "?CosAnneal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47895aca47194f11bdc75869a2fbe9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.187258   1.31315   \n",
      "    1      1.186601   1.313008                               \n",
      "    2      1.186906   1.312391                               \n",
      "    3      1.187094   1.313229                               \n",
      "    4      1.190092   1.312537                               \n",
      "    5      1.18726    1.311848                               \n",
      "    6      1.1829     1.311563                               \n",
      "    7      1.187133   1.312235                               \n",
      "    8      1.186393   1.312395                               \n",
      "    9      1.185543   1.312864                               \n",
      "    10     1.186253   1.312902                               \n",
      "    11     1.181841   1.312306                               \n",
      "    12     1.182119   1.312219                               \n",
      "    13     1.177267   1.312097                               \n",
      "    14     1.175407   1.311875                               \n",
      "    15     1.179057   1.313027                               \n",
      "    16     1.177683   1.313203                               \n",
      "    17     1.182339   1.312788                               \n",
      "    18     1.174631   1.312774                               \n",
      "    19     1.175498   1.312542                               \n",
      "    20     1.172381   1.313432                               \n",
      "    21     1.173432   1.312918                               \n",
      "    22     1.170284   1.312995                               \n",
      "    23     1.171553   1.312383                               \n",
      "    24     1.170774   1.312752                               \n",
      "    25     1.168628   1.312759                               \n",
      "    26     1.161744   1.31299                                \n",
      "    27     1.166072   1.312445                               \n",
      "    28     1.167197   1.312327                               \n",
      "    29     1.15871    1.312596                               \n",
      "    30     1.160179   1.31262                                \n",
      "    31     1.166965   1.312555                               \n",
      "    32     1.170378   1.313879                               \n",
      "    33     1.170302   1.314532                               \n",
      "    34     1.170076   1.314224                               \n",
      "    35     1.164737   1.314654                               \n",
      "    36     1.169252   1.314607                               \n",
      "    37     1.16169    1.314567                               \n",
      "    38     1.166725   1.314584                               \n",
      "    39     1.162783   1.314684                               \n",
      "    40     1.161709   1.315039                               \n",
      "    41     1.157961   1.315364                               \n",
      "    42     1.156482   1.314877                               \n",
      "    43     1.162534   1.314822                               \n",
      "    44     1.157567   1.315079                               \n",
      "    45     1.15886    1.315349                               \n",
      "    46     1.150328   1.315779                               \n",
      "    47     1.15389    1.316091                               \n",
      "    48     1.153171   1.315341                               \n",
      "    49     1.150494   1.316034                               \n",
      "    50     1.154063   1.315465                               \n",
      "    51     1.154578   1.315973                               \n",
      "    52     1.153209   1.315751                               \n",
      "    53     1.145811   1.316059                               \n",
      "    54     1.152413   1.315875                               \n",
      "    55     1.149312   1.315714                               \n",
      "    56     1.147556   1.315688                               \n",
      "    57     1.147923   1.315636                               \n",
      "    58     1.149887   1.315821                               \n",
      "    59     1.138962   1.315796                               \n",
      "    60     1.144173   1.315756                               \n",
      "    61     1.147639   1.315843                               \n",
      "    62     1.147564   1.315658                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.31566])]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Call backs to do Cosine Annealing\n",
    "on_end = lambda sched, cycle: save_model(model, f'{PATH}models/cycle_{cycle}') #Save model\n",
    "call_back = [CosAnneal(lo, len(model_data.trn_dl), cycle_mult=2, on_cycle_end=on_end)] #trn_dl is length of the dataloader\n",
    "fit(model, model_data, 2**6-1, lo.opt, F.nll_loss, callbacks = call_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = model(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those who, as an investignors intoo future. learned beaut of their genesis a german quite to be a timeof the habit so long of the lurrowers, amorn of feelings ins\"faces.\" first and down the dreams: one has been here.47. perhaps pleasure in3rage that they arethe thinking of all that comes, indeed it con taste. in one--\"what they have, holy advincious more their so far to itself is almost by itself--how\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
